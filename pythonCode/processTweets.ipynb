{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "\n",
    "def mergeTweets (input_folder, selected_cols):\n",
    "    files = [f for f in listdir(input_folder) if isfile(join(input_folder, f))] # can exclude non-files\n",
    "    files.sort()\n",
    "    #files = files[:3]\n",
    "    result = None\n",
    "    for f in files:\n",
    "        input_file = join(input_folder, f)\n",
    "        df = pd.read_csv(input_file, dtype=str, index_col=None)\n",
    "        df = df.loc[:,selected_cols]\n",
    "        result = pd.concat([result,df])\n",
    "        #print(input_file)\n",
    "    return result    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t01_part1 is done.\n",
      "t01_part2 is done.\n",
      "t01_part3 is done.\n",
      "t01_part4 is done.\n",
      "t02_part1 is done.\n",
      "t02_part2 is done.\n",
      "t02_part3 is done.\n",
      "t02_part4 is done.\n",
      "t03_part1 is done.\n",
      "t03_part2 is done.\n",
      "t03_part3 is done.\n",
      "t03_part4 is done.\n",
      "t04_part1 is done.\n",
      "t04_part2 is done.\n",
      "t04_part3 is done.\n",
      "t04_part4 is done.\n",
      "t05_part1 is done.\n",
      "t05_part2 is done.\n",
      "t05_part3 is done.\n",
      "t05_part4 is done.\n",
      "t06_part1 is done.\n",
      "t06_part2 is done.\n",
      "t06_part3 is done.\n",
      "t06_part4 is done.\n",
      "t07_part1 is done.\n",
      "t07_part2 is done.\n",
      "t07_part3 is done.\n",
      "t07_part4 is done.\n",
      "t08_part1 is done.\n",
      "t08_part2 is done.\n",
      "t08_part3 is done.\n",
      "t08_part4 is done.\n",
      "t09_part1 is done.\n",
      "t09_part2 is done.\n",
      "t09_part3 is done.\n",
      "t09_part4 is done.\n",
      "t10_part1 is done.\n",
      "t10_part2 is done.\n",
      "t10_part3 is done.\n",
      "t10_part4 is done.\n",
      "t11_part1 is done.\n",
      "t11_part2 is done.\n",
      "t11_part3 is done.\n",
      "t11_part4 is done.\n",
      "t12_part1 is done.\n",
      "t12_part2 is done.\n",
      "t12_part3 is done.\n",
      "t12_part4 is done.\n",
      "t13_part1 is done.\n",
      "t13_part2 is done.\n",
      "t13_part3 is done.\n",
      "t13_part4 is done.\n",
      "t14_part1 is done.\n",
      "t14_part2 is done.\n",
      "t14_part3 is done.\n",
      "t14_part4 is done.\n"
     ]
    }
   ],
   "source": [
    "folders = listdir(\"../../data/friends_info/edgelist_Feb27/timelines_csv/\")\n",
    "folders.sort()\n",
    "selected_cols = [\"user_id_str\",\"id_str\",\"created_at\", \"text\"]\n",
    "\n",
    "for ff in folders:\n",
    "    input_folder = join(\"../../data/friends_info/edgelist_Feb27/timelines_csv/\", ff)\n",
    "    output_file = \"../../data/friends_info/edgelist_Feb27/timelines_csv_simplified/\"+ff+\".csv\"\n",
    "    result = mergeTweets(input_folder, selected_cols)\n",
    "    result.to_csv(output_file) \n",
    "    print(ff+\" is done.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['118.csv', '036.csv', '047.csv', '095.csv', '015.csv', '057.csv', '082.csv', '123.csv', '121.csv', '035.csv', '050.csv', '005.csv', '058.csv', '048.csv', '072.csv', '049.csv', '012.csv', '083.csv', '062.csv', '001.csv', '002.csv', '130.csv', '109.csv', '039.csv', '059.csv', '066.csv', '111.csv', '080.csv', '017.csv', '108.csv', '087.csv', '102.csv', '031.csv', '132.csv', '104.csv', '103.csv', '106.csv', '027.csv', '107.csv', '019.csv', '093.csv', '041.csv', '088.csv', '098.csv', '011.csv', '028.csv', '029.csv', '129.csv', '079.csv', '063.csv', '094.csv', '034.csv', '006.csv', '054.csv', '073.csv', '116.csv', '127.csv', '007.csv', '038.csv', '055.csv', '074.csv', '053.csv', '023.csv', '115.csv', '137.csv', '133.csv', '061.csv', '010.csv', '097.csv', '021.csv', '124.csv', '004.csv', '024.csv', '022.csv', '089.csv', '046.csv', '013.csv', '008.csv', '030.csv', '086.csv', '056.csv', '096.csv', '065.csv', '078.csv', '090.csv', '040.csv', '075.csv', '101.csv', '131.csv', '064.csv', '119.csv', '052.csv', '033.csv', '067.csv', '069.csv', '128.csv', '014.csv', '071.csv', '032.csv', '026.csv', '077.csv', '043.csv', '025.csv', '136.csv', '126.csv', '070.csv', '084.csv', '003.csv', '092.csv', '100.csv', '051.csv', '114.csv', '113.csv', '091.csv', '135.csv', '044.csv', '105.csv', '020.csv', '081.csv', '120.csv', '122.csv', '125.csv', '009.csv', '018.csv', '085.csv', '016.csv', '037.csv', '045.csv', '117.csv', '076.csv', '060.csv', '110.csv', '042.csv', '068.csv', '112.csv', '134.csv', '099.csv']\n",
      "['001.csv', '002.csv', '003.csv', '004.csv', '005.csv', '006.csv', '007.csv', '008.csv', '009.csv', '010.csv', '011.csv', '012.csv', '013.csv', '014.csv', '015.csv', '016.csv', '017.csv', '018.csv', '019.csv', '020.csv', '021.csv', '022.csv', '023.csv', '024.csv', '025.csv', '026.csv', '027.csv', '028.csv', '029.csv', '030.csv', '031.csv', '032.csv', '033.csv', '034.csv', '035.csv', '036.csv', '037.csv', '038.csv', '039.csv', '040.csv', '041.csv', '042.csv', '043.csv', '044.csv', '045.csv', '046.csv', '047.csv', '048.csv', '049.csv', '050.csv', '051.csv', '052.csv', '053.csv', '054.csv', '055.csv', '056.csv', '057.csv', '058.csv', '059.csv', '060.csv', '061.csv', '062.csv', '063.csv', '064.csv', '065.csv', '066.csv', '067.csv', '068.csv', '069.csv', '070.csv', '071.csv', '072.csv', '073.csv', '074.csv', '075.csv', '076.csv', '077.csv', '078.csv', '079.csv', '080.csv', '081.csv', '082.csv', '083.csv', '084.csv', '085.csv', '086.csv', '087.csv', '088.csv', '089.csv', '090.csv', '091.csv', '092.csv', '093.csv', '094.csv', '095.csv', '096.csv', '097.csv', '098.csv', '099.csv', '100.csv', '101.csv', '102.csv', '103.csv', '104.csv', '105.csv', '106.csv', '107.csv', '108.csv', '109.csv', '110.csv', '111.csv', '112.csv', '113.csv', '114.csv', '115.csv', '116.csv', '117.csv', '118.csv', '119.csv', '120.csv', '121.csv', '122.csv', '123.csv', '124.csv', '125.csv', '126.csv', '127.csv', '128.csv', '129.csv', '130.csv', '131.csv', '132.csv', '133.csv', '134.csv', '135.csv', '136.csv', '137.csv']\n"
     ]
    }
   ],
   "source": [
    "files = listdir(input_folder)\n",
    "type (files)\n",
    "print(files)\n",
    "files.sort()\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(109463, 23)\n",
      "109463\n",
      "Index(['created_at', 'id_str', 'text', 'truncated', 'source',\n",
      "       'in_reply_to_status_id_str', 'in_reply_to_user_id_str',\n",
      "       'quoted_status_created_at', 'quoted_status_id_str',\n",
      "       'quoted_status_text', 'quoted_status_user_id_str',\n",
      "       'retweet_status_id_str', 'retweet_status_created_at',\n",
      "       'retweet_status_text', 'retweet_status_user_id_str', 'user_id_str',\n",
      "       'place_name', 'place_country', 'retweet_count', 'favorite_count',\n",
      "       'favorited', 'retweeted', 'lang'],\n",
      "      dtype='object')\n",
      "RangeIndex(start=0, stop=10, step=1)\n"
     ]
    }
   ],
   "source": [
    "#some tutorials \n",
    "files = listdir(input_folder)\n",
    "isfile(join(input_folder, files[0]))\n",
    "\n",
    "df1 = pd.read_csv(input_file)\n",
    "# these are property of the dataframe\n",
    "print(df.shape)\n",
    "print(len(df.index)) \n",
    "print(df.columns)\n",
    "print(df.index[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#df2.to_csv(\"../../data/friends_info/edgelist_Feb27/timelines_csv/t01_part2/001_00.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "selected_cols = [\"user_id_str\",\"id_str\",\"created_at\", \"text\"]\n",
    "df1 = df.loc[:,selected_cols]\n",
    "#df1.to_csv(\"../../data/friends_info/edgelist_Feb27/timelines_csv/t01_part2/001_00.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(212780, 4)\n"
     ]
    }
   ],
   "source": [
    "#print(df1.iloc[:10,:])\n",
    "#print(df2.iloc[:10,:])\n",
    "df3 = pd.concat([df1, df2])\n",
    "print(df3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df4 =None\n",
    "df5 = pd.concat([df3,df4])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
