{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# { 'default_profile': [False], 'profile_background_color': ['1A1B1F'], 'description': [''], 'location': ['Duluth/Atlanta, GA'], 'id_str': ['443226440'], 'contributors_enabled': [False], 'followers_count': [16], 'friends_count': [127], 'name': ['Tony Schlitt'], 'utc_offset': [-14400], 'profile_sidebar_border_color': ['181A1E'], 'profile_background_image_url_https': ['https://abs.twimg.com/images/themes/theme9/bg.gif'], 'id': [443226440], 'favourites_count': [226], 'profile_use_background_image': [True], 'status': { 'in_reply_to_status_id': {}, 'id_str': ['783437675328135173'], 'retweet_count': [1], 'is_quote_status': [False], 'retweeted': [False], 'in_reply_to_user_id': [442246443], 'in_reply_to_screen_name': ['brianhoyt24'], 'truncated': [False], 'source': ['Twitter for iPhone'], 'favorite_count': [1], 'id': [7.83437675328135e+17], 'geo': {}, 'contributors': {}, 'lang': ['en'], 'favorited': [False], 'created_at': ['Tue Oct 04 22:44:32 +0000 2016'], 'text': ['@brianhoyt24 @FrontOfficeLos I did the same as Hoyt üò¢, but you have to play a \"Woo! off\" between Hoyt and Dimino\\'s woos! üòÇüòÇüòÇ'], 'in_reply_to_user_id_str': ['442246443'], 'coordinates': {}, 'in_reply_to_status_id_str': {}, 'entities': { 'hashtags': [], 'symbols': [], 'user_mentions': [{ 'indices': [ [0], [12] ], 'screen_name': ['brianhoyt24'], 'name': ['Happy Hoyt'], 'id': [442246443], 'id_str': ['442246443'] }, { 'indices': [ [13], [28] ], 'screen_name': ['FrontOfficeLos'], 'name': ['Los Medina'], 'id': [487944445], 'id_str': ['487944445'] }], 'urls': [] }, 'place': { 'bounding_box': { 'type': ['Polygon'], 'coordinates': [ [ [ [-85.6052], [30.3556] ], [ [-80.7426], [30.3556] ], [ [-80.7426], [35.0008] ], [ [-85.6052], [35.0008] ] ] ] }, 'place_type': ['admin'], 'attributes': {}, 'country': ['United States'], 'contained_within': [], 'id': ['7142eb97ae21e839'], 'country_code': ['US'], 'url': ['https://api.twitter.com/1.1/geo/id/7142eb97ae21e839.json'], 'name': ['Georgia'], 'full_name': ['Georgia, USA'] } }, 'profile_image_url': ['http://pbs.twimg.com/profile_images/1883818109/Al3rB_CCIAAfSdq_normal.jpg'], 'verified': [False], 'is_translator': [False], 'created_at': ['Wed Dec 21 23:35:27 +0000 2011'], 'profile_background_image_url': ['http://abs.twimg.com/images/themes/theme9/bg.gif'], 'url': {}, 'profile_link_color': ['2FC2EF'], 'time_zone': ['Eastern Time (US & Canada)'], 'is_translation_enabled': [False], 'statuses_count': [600], 'has_extended_profile': [False], 'profile_sidebar_fill_color': ['252429'], 'notifications': [False], 'default_profile_image': [False], 'screen_name': ['printpirate'], 'geo_enabled': [True], 'lang': ['en'], 'listed_count': [0], 'following': [False], 'profile_image_url_https': ['https://pbs.twimg.com/profile_images/1883818109/Al3rB_CCIAAfSdq_normal.jpg'], 'entities': { 'description': { 'urls': [] } }, 'follow_request_sent': [False], 'protected': [False], 'profile_background_tile': [False], 'profile_text_color': ['666666'] }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"# extract the given column info, produce a dataframe\\n\",\n",
    "\"#Note :1, the field in user json can be dict or list. this function not handle dict now.\\n\",\n",
    "\"#      the NULL, will give a complete graph, which expand all the dict as well.       \\n\",\n",
    "\"#      2, some lines of the downloaded json items are not valid, use try except to get rid of them\\n\",\n",
    "\n",
    "import re\n",
    "import json\n",
    "import pandas as pd \n",
    "import csv\n",
    "   \n",
    "\n",
    "def json2dataframe(json_file, cols = None, verbose = False):\n",
    "    \"\"\"\n",
    "    json_file, a filepath to a json file containing multiple json objects\n",
    "    cols: extract the selected keys, or extract all the keys, and return a dataframe\n",
    "    verbose: to output the nuber of invalid json lines\n",
    "    rtype: panda DataFrame\n",
    "    \"\"\"\n",
    "    if cols:\n",
    "        data = {}\n",
    "        n_invalids = 0\n",
    "        for c in cols:\n",
    "            data[c] = []\n",
    "        with open (json_file) as f1:\n",
    "            for line in f1:\n",
    "                try:\n",
    "                    line = json.loads(line) #case 1, not valid json, pass\n",
    "                except:\n",
    "                    n_invalids += 1    \n",
    "                    continue\n",
    "                if 'id_str' not in line:   #case 2, valid.json stype, no id_str, pass\n",
    "                    continue\n",
    "                for c in cols:             \n",
    "                    if c not in line or isinstance(line[c], dict) :\n",
    "                        data[c].append('NA_VALUE')\n",
    "                    elif  line[c] == []: #include empty list\n",
    "                        data[c].extend('NA_VALUE') \n",
    "                    else:\n",
    "                        aa = str(line[c][0])\n",
    "                        data[c].append( re.sub(\"\\\\s+\",\" \",re.sub(\"[\\n\\r\\t]+\", \" \", aa)) )  \n",
    "        data_df = pd.DataFrame.from_dict(data)\n",
    "    else:\n",
    "        json_list = []\n",
    "        n_invalids = 0\n",
    "        with open (json_file) as f1:\n",
    "            for line in f1:\n",
    "                try:\n",
    "                    line = json.loads(line)\n",
    "                    json_list.append(line)\n",
    "                except:\n",
    "                    n_invalids += 1\n",
    "                    continue           \n",
    "        data_df = pd.DataFrame(json_list) \n",
    "    if verbose: print(\"number of invalid lines removed: \" + str(n_invalids))\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "16\n",
      "number of invalid lines removed: 0\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "#example on small json file\n",
    "cols = ['id_str', 'screen_name', 'name','created_at', 'description', \n",
    "        'location', 'time_zone',  'favourites_count','followers_count',  'friends_count',\n",
    "        'lang', 'listed_count' ,'statuses_count', 'geo_enabled', 'protected' ,'verified'] \n",
    "path = \"../../data/followers_info/followers10.json\"\n",
    "cols1 = ['id_str', 'screen_name','description','followers_count']\n",
    "toys_df1 = json2dataframe(path, cols1)\n",
    "print(len(toys_df1.columns))\n",
    "\n",
    "cols = ['id_str', 'screen_name', 'name','created_at', 'description', \n",
    "        'location', 'time_zone',  'favourites_count','followers_count',  'friends_count',\n",
    "        'lang', 'listed_count' ,'statuses_count', 'geo_enabled', 'protected' ,'verified'] \n",
    "toys_df2 = json2dataframe(path, cols)\n",
    "print(len(toys_df2.columns))\n",
    "\n",
    "toys_df3 = json2dataframe(path, verbose = True)\n",
    "print(len(toys_df3.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>description</th>\n",
       "      <th>favourites_count</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>geo_enabled</th>\n",
       "      <th>id_str</th>\n",
       "      <th>lang</th>\n",
       "      <th>listed_count</th>\n",
       "      <th>location</th>\n",
       "      <th>name</th>\n",
       "      <th>protected</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>statuses_count</th>\n",
       "      <th>time_zone</th>\n",
       "      <th>verified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sun Jul 03 21:09:26 +0000 2016</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>52</td>\n",
       "      <td>False</td>\n",
       "      <td>749711671782961152</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>mungai simon ngigi</td>\n",
       "      <td>False</td>\n",
       "      <td>mungaisimonngi2</td>\n",
       "      <td>0</td>\n",
       "      <td>NA_VALUE</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wed Aug 31 01:01:00 +0000 2016</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>False</td>\n",
       "      <td>770788443609829376</td>\n",
       "      <td>ru</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>–ê–Ω–∞—Å—Ç–∞—Å–∏—è –ê–Ω—Ç–æ–Ω–æ–≤–∞</td>\n",
       "      <td>False</td>\n",
       "      <td>dyachkovalyuci6</td>\n",
       "      <td>0</td>\n",
       "      <td>NA_VALUE</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fri Sep 16 14:24:48 +0000 2016</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>False</td>\n",
       "      <td>776788934177292288</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>Donna Tyrone</td>\n",
       "      <td>False</td>\n",
       "      <td>tyrone_donna</td>\n",
       "      <td>1</td>\n",
       "      <td>NA_VALUE</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thu Jul 21 15:45:15 +0000 2016</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>False</td>\n",
       "      <td>756153070594039808</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>Chappy Politics</td>\n",
       "      <td>True</td>\n",
       "      <td>ChappyPolitics</td>\n",
       "      <td>0</td>\n",
       "      <td>NA_VALUE</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tue May 29 20:37:42 +0000 2012</td>\n",
       "      <td>I am very much friendly and jovial and have a ...</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>335</td>\n",
       "      <td>True</td>\n",
       "      <td>594099777</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>India</td>\n",
       "      <td>Sarat Banerjee</td>\n",
       "      <td>False</td>\n",
       "      <td>banerjeesarat</td>\n",
       "      <td>31</td>\n",
       "      <td>Kolkata</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sat Jul 18 10:43:30 +0000 2015</td>\n",
       "      <td>ACHS Freshman</td>\n",
       "      <td>209</td>\n",
       "      <td>24</td>\n",
       "      <td>91</td>\n",
       "      <td>False</td>\n",
       "      <td>3381564803</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>Lawrenceburg, KY</td>\n",
       "      <td>Ethan Smith</td>\n",
       "      <td>False</td>\n",
       "      <td>EthanSm95233995</td>\n",
       "      <td>0</td>\n",
       "      <td>NA_VALUE</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tue Jul 12 13:06:30 +0000 2016</td>\n",
       "      <td>Love taking pictures‚ù§ Single Fun Love shopping...</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>102</td>\n",
       "      <td>False</td>\n",
       "      <td>752851627720241152</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>Loganville, GA</td>\n",
       "      <td>Troyann Campbell</td>\n",
       "      <td>False</td>\n",
       "      <td>Thickbitvht</td>\n",
       "      <td>2</td>\n",
       "      <td>NA_VALUE</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Mon May 16 17:53:18 +0000 2016</td>\n",
       "      <td>I am an Oklahoma State University grad. I have...</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "      <td>False</td>\n",
       "      <td>732267694855053313</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>Oklahoma, USA</td>\n",
       "      <td>Cassnloganmom</td>\n",
       "      <td>False</td>\n",
       "      <td>AbrahamLincln82</td>\n",
       "      <td>0</td>\n",
       "      <td>NA_VALUE</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Wed Sep 14 13:30:02 +0000 2016</td>\n",
       "      <td>#Election2016 Art Arts &amp; Culture Business &amp; Fi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>False</td>\n",
       "      <td>776050374914535425</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>Nashville, TN</td>\n",
       "      <td>Qisen Song</td>\n",
       "      <td>False</td>\n",
       "      <td>nealolaen</td>\n",
       "      <td>0</td>\n",
       "      <td>NA_VALUE</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sun Jul 03 14:27:43 +0000 2016</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>122</td>\n",
       "      <td>False</td>\n",
       "      <td>749610578121854976</td>\n",
       "      <td>vi</td>\n",
       "      <td>0</td>\n",
       "      <td>ÈùíÊ£Æ ‰∫îÊâÄÂ∑ùÂéüÂ∏Ç</td>\n",
       "      <td>Ho√†ng Th·ªã Y·∫øn</td>\n",
       "      <td>False</td>\n",
       "      <td>Hg1Yen</td>\n",
       "      <td>0</td>\n",
       "      <td>NA_VALUE</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       created_at  \\\n",
       "0  Sun Jul 03 21:09:26 +0000 2016   \n",
       "1  Wed Aug 31 01:01:00 +0000 2016   \n",
       "2  Fri Sep 16 14:24:48 +0000 2016   \n",
       "3  Thu Jul 21 15:45:15 +0000 2016   \n",
       "4  Tue May 29 20:37:42 +0000 2012   \n",
       "5  Sat Jul 18 10:43:30 +0000 2015   \n",
       "6  Tue Jul 12 13:06:30 +0000 2016   \n",
       "7  Mon May 16 17:53:18 +0000 2016   \n",
       "8  Wed Sep 14 13:30:02 +0000 2016   \n",
       "9  Sun Jul 03 14:27:43 +0000 2016   \n",
       "\n",
       "                                         description favourites_count  \\\n",
       "0                                                                   0   \n",
       "1                                                                   0   \n",
       "2                                                                   0   \n",
       "3                                                                   0   \n",
       "4  I am very much friendly and jovial and have a ...               19   \n",
       "5                                      ACHS Freshman              209   \n",
       "6  Love taking pictures‚ù§ Single Fun Love shopping...                0   \n",
       "7  I am an Oklahoma State University grad. I have...                8   \n",
       "8  #Election2016 Art Arts & Culture Business & Fi...                0   \n",
       "9                                                                   0   \n",
       "\n",
       "  followers_count friends_count geo_enabled              id_str lang  \\\n",
       "0              10            52       False  749711671782961152   en   \n",
       "1               3            19       False  770788443609829376   ru   \n",
       "2               1            25       False  776788934177292288   en   \n",
       "3               0            19       False  756153070594039808   en   \n",
       "4              15           335        True           594099777   en   \n",
       "5              24            91       False          3381564803   en   \n",
       "6               8           102       False  752851627720241152   en   \n",
       "7               2            40       False  732267694855053313   en   \n",
       "8               0            36       False  776050374914535425   en   \n",
       "9              19           122       False  749610578121854976   vi   \n",
       "\n",
       "  listed_count          location                name protected  \\\n",
       "0            0                    mungai simon ngigi     False   \n",
       "1            0                    –ê–Ω–∞—Å—Ç–∞—Å–∏—è –ê–Ω—Ç–æ–Ω–æ–≤–∞     False   \n",
       "2            0                          Donna Tyrone     False   \n",
       "3            0                       Chappy Politics      True   \n",
       "4            0             India      Sarat Banerjee     False   \n",
       "5            1  Lawrenceburg, KY         Ethan Smith     False   \n",
       "6            0    Loganville, GA    Troyann Campbell     False   \n",
       "7            0     Oklahoma, USA       Cassnloganmom     False   \n",
       "8            0     Nashville, TN          Qisen Song     False   \n",
       "9            0          ÈùíÊ£Æ ‰∫îÊâÄÂ∑ùÂéüÂ∏Ç       Ho√†ng Th·ªã Y·∫øn     False   \n",
       "\n",
       "       screen_name statuses_count time_zone verified  \n",
       "0  mungaisimonngi2              0  NA_VALUE    False  \n",
       "1  dyachkovalyuci6              0  NA_VALUE    False  \n",
       "2     tyrone_donna              1  NA_VALUE    False  \n",
       "3   ChappyPolitics              0  NA_VALUE    False  \n",
       "4    banerjeesarat             31   Kolkata    False  \n",
       "5  EthanSm95233995              0  NA_VALUE    False  \n",
       "6      Thickbitvht              2  NA_VALUE    False  \n",
       "7  AbrahamLincln82              0  NA_VALUE    False  \n",
       "8        nealolaen              0  NA_VALUE    False  \n",
       "9           Hg1Yen              0  NA_VALUE    False  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#toys_df1.to_csv(\"followers10.csv\", header = False)\n",
    "toys_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of invalid lines removed: 2\n",
      "i=0 is done , sample size: 803920\n",
      "number of invalid lines removed: 1\n",
      "i=1 is done , sample size: 949999\n",
      "number of invalid lines removed: 0\n",
      "i=2 is done , sample size: 949999\n",
      "number of invalid lines removed: 3\n",
      "i=3 is done , sample size: 949996\n",
      "number of invalid lines removed: 3\n",
      "i=4 is done , sample size: 949997\n",
      "number of invalid lines removed: 3\n",
      "i=5 is done , sample size: 850202\n",
      "number of invalid lines removed: 0\n",
      "i=6 is done , sample size: 933486\n",
      "number of invalid lines removed: 1\n",
      "i=7 is done , sample size: 800000\n",
      "number of invalid lines removed: 0\n",
      "i=8 is done , sample size: 800000\n",
      "number of invalid lines removed: 1\n",
      "i=9 is done , sample size: 800000\n",
      "number of invalid lines removed: 1\n",
      "i=10 is done , sample size: 800000\n",
      "number of invalid lines removed: 1\n",
      "i=11 is done , sample size: 800000\n",
      "number of invalid lines removed: 1\n",
      "i=12 is done , sample size: 800000\n",
      "number of invalid lines removed: 0\n",
      "i=13 is done , sample size: 800000\n",
      "number of invalid lines removed: 0\n",
      "i=14 is done , sample size: 800000\n",
      "number of invalid lines removed: 0\n",
      "i=15 is done , sample size: 91887\n"
     ]
    }
   ],
   "source": [
    "#feb 17, 2017\n",
    "# Mar22,\n",
    "# MAr29\n",
    "# April 13\n",
    "#cols = ['id_str', 'screen_name','description','followers_count']\n",
    "cols = ['id_str', 'screen_name', 'name','created_at', 'description', \n",
    "        'location', 'time_zone',  'favourites_count','followers_count',  'friends_count',\n",
    "        'lang', 'listed_count' ,'statuses_count', 'geo_enabled', 'protected' ,'verified'] \n",
    "#decription, need to do text cleaning before write into .csv\n",
    "for i in range(0,16):\n",
    "    if i < 10:\n",
    "        inputpath = \"../../data/followers_info/jsons/trump0\"+str(i)+\".json\"\n",
    "        outputpath = \"../../data/followers_info/csvs/trump0\"+str(i)+\".csv\"\n",
    "    elif i < 100:\n",
    "        inputpath = \"../../data/followers_info/jsons/trump\"+str(i)+\".json\"\n",
    "        outputpath = \"../../data/followers_info/csvs/trump\"+str(i)+\".csv\"\n",
    "    trump_df = json2dataframe(inputpath, cols, verbose = True)  \n",
    "    print (\"i=\" +  str(i) +\" is done , sample size: \" + str(len(trump_df)))\n",
    "    trump_df.to_csv(outputpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "number of invalid lines removed: 2\n",
    "i=0 is done , sample size: 803920\n",
    "number of invalid lines removed: 1\n",
    "i=1 is done , sample size: 949999\n",
    "number of invalid lines removed: 0\n",
    "i=2 is done , sample size: 949999\n",
    "number of invalid lines removed: 3\n",
    "i=3 is done , sample size: 949996\n",
    "number of invalid lines removed: 3\n",
    "i=4 is done , sample size: 949997\n",
    "number of invalid lines removed: 3\n",
    "i=5 is done , sample size: 850202\n",
    "number of invalid lines removed: 0\n",
    "i=6 is done , sample size: 933486\n",
    "number of invalid lines removed: 1\n",
    "i=7 is done , sample size: 800000\n",
    "number of invalid lines removed: 0\n",
    "i=8 is done , sample size: 800000\n",
    "number of invalid lines removed: 1\n",
    "i=9 is done , sample size: 800000\n",
    "number of invalid lines removed: 1\n",
    "i=10 is done , sample size: 800000\n",
    "number of invalid lines removed: 1\n",
    "i=11 is done , sample size: 800000\n",
    "number of invalid lines removed: 1\n",
    "i=12 is done , sample size: 800000\n",
    "number of invalid lines removed: 0\n",
    "i=13 is done , sample size: 800000\n",
    "number of invalid lines removed: 0\n",
    "i=14 is done , sample size: 800000\n",
    "number of invalid lines removed: 0\n",
    "i=15 is done , sample size: 91887\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of invalid lines removed: 68\n",
      "sample size: 1944633\n",
      "number of invalid lines removed: 138\n",
      "sample size: 4100181\n",
      "number of invalid lines removed: 117\n",
      "sample size: 3160939\n"
     ]
    }
   ],
   "source": [
    "# process all the friends.json, in three graphs\n",
    "# paril 17, 2017\n",
    "\n",
    "#cols = ['id_str', 'screen_name','description','followers_count']\n",
    "cols = ['id_str', 'screen_name', 'name','created_at', 'description', \n",
    "        'location', 'time_zone',  'favourites_count','followers_count',  'friends_count',\n",
    "        'lang', 'listed_count' ,'statuses_count', 'geo_enabled', 'protected' ,'verified'] \n",
    "\n",
    "\n",
    "inputpath = \"../../data/friends_info/data_friends/friends_1.json\"\n",
    "outputpath = \"../../data/friends_info/data_friends/friends_1_info.csv\"\n",
    "df = json2dataframe(inputpath, cols, verbose = True)  \n",
    "print (\"sample size: \" + str(len(df)))\n",
    "df.to_csv(outputpath)\n",
    "\n",
    "\n",
    "inputpath = \"../../data/friends_info/data_friends/friends_2.json\"\n",
    "outputpath = \"../../data/friends_info/data_friends/friends_2_info.csv\"\n",
    "df = json2dataframe(inputpath, cols, verbose = True)  \n",
    "print (\"sample size: \" + str(len(df)))\n",
    "df.to_csv(outputpath)\n",
    "          \n",
    "inputpath = \"../../data/friends_info/data_friends/friends_3.json\"\n",
    "outputpath = \"../../data/friends_info/data_friends/friends_3_info.csv\"\n",
    "df = json2dataframe(inputpath, cols, verbose = True)  \n",
    "print (\"sample size: \" + str(len(df)))\n",
    "df.to_csv(outputpath)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "number of invalid lines removed: 68\n",
    "sample size: 1944633\n",
    "number of invalid lines removed: 138\n",
    "sample size: 4100181\n",
    "number of invalid lines removed: 117\n",
    "sample size: 3160939\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
